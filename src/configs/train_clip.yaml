paths:
  dataset-root: "/Users/pazimor/dataset_preprocessed"
  checkpoint-dir: output/clip
  # Network architecture is loaded from network.yaml
  network-config: "src/configs/network.yaml"

# ============================================
# Profile: Mac (32GB RAM, MPS GPU)
# ============================================
training:
  epochs: 500
  device: mps
  max-length: 256
  model-name: xlm-roberta-base
  validation-split: 0.1
  early-stopping-patience: 100
  batch-size: 8
  gradient-accumulation: 16
  MM-memory-limit-gb: 25.0
  weight-decay: 0.01
  # resume-checkpoint: output/clip/best_model.pt
  
  # Learning Rate Configuration
  learning-rate: 0.0001      # Initial/base learning rate
  lr-min: 0.0000001           # Minimum LR floor (1e-7)
  lr-warmup-epochs: 3         # Warmup epochs (0 to disable)
  lr-schedule: cosine         # Schedule type: constant, cosine, linear, step
  # lr-decay-epochs: 90       # Optional: decay phase length

# ============================================
# Profile: DGX Spark (128GB RAM, GB10 GPU)
# Use with: --profile spark
# ============================================
spark:
  epochs: 100
  device: cuda
  max-length: 256
  model-name: xlm-roberta-base
  validation-split: 0.1
  early-stopping-patience: 15
  batch-size: 64
  gradient-accumulation: 2
  MM-memory-limit-gb: 0.0
  weight-decay: 0.01
  
  # Learning Rate Configuration
  learning-rate: 0.0001
  lr-min: 0.000001
  lr-warmup-epochs: 5
  lr-schedule: cosine
